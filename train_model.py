import os
import copy
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# ============================================================
# 1. CONFIGURATION (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤)
# ============================================================
# üî• ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Path ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
DATA_DIR = r"C:\Users\rutsa\PycharmProjects\MLproject\MLproject\dataset"
MODEL_SAVE_PATH = 'dog_model_pytorch.pth'

IMG_SIZE = 224
BATCH_SIZE = 16  # ‡∏•‡∏î‡∏•‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Fine-tuning ‡πÑ‡∏î‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
EPOCHS = 20  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
LEARNING_RATE = 1e-4  # üî• ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÉ‡∏ä‡πâ LR ‡∏ï‡πà‡∏≥‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏à‡∏π‡∏ô (Fine-tuning)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Using device: {device}")

# ============================================================
# 2. DATA AUGMENTATION (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÇ‡∏à‡∏ó‡∏¢‡πå)
# ============================================================
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        # ‡∏™‡∏∏‡πà‡∏°‡∏û‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û/‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (‡∏≠‡∏¢‡πà‡∏≤‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏á‡∏á‡πÄ‡∏á‡∏≤)
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        # üî• ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏µ/‡πÅ‡∏™‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÅ‡∏Ñ‡πà‡∏™‡∏µ ‡πÅ‡∏ï‡πà‡∏î‡∏π texture
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}


# ============================================================
# 3. PROCESS
# ============================================================
def load_data():
    full_dataset = datasets.ImageFolder(DATA_DIR, transform=data_transforms['train'])
    class_names = full_dataset.classes
    print(f"‚úÖ Classes: {class_names}")

    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ñ‡∏π‡∏Å‡πÑ‡∏´‡∏° (0=ai, 1=real ‡∏õ‡∏Å‡∏ï‡∏¥‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
    if class_names[0] != 'ai':
        print("‚ö†Ô∏è Warning: Class order might be unexpected. Check folder names.")

    # Split Data (70/15/15)
    total_size = len(full_dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(42)
    )

    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Transform ‡∏Ç‡∏≠‡∏á Val/Test ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏¥‡πà‡∏á‡πÜ (‡πÑ‡∏°‡πà‡∏°‡∏µ Random)
    val_dataset.dataset.transform = data_transforms['val']
    test_dataset.dataset.transform = data_transforms['val']

    dataloaders = {
        'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0),
        'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0),
        'test': DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    }

    return dataloaders, class_names, len(train_dataset), len(val_dataset)


def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    # ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏ß‡πâ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch + 1}/{num_epochs}')
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # Loop ‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            for inputs, labels in tqdm(dataloaders[phase], desc=f"{phase}"):
                inputs = inputs.to(device)
                labels = labels.to(device).float().unsqueeze(1)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    preds = (torch.sigmoid(outputs) > 0.5).float()

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase == 'val':
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())
                # ‡∏õ‡∏£‡∏±‡∏ö Learning Rate ‡∏ñ‡πâ‡∏≤ loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á
                scheduler.step(epoch_loss)

                # üî• Save Best Model Only
                if epoch_acc > best_acc:
                    best_acc = epoch_acc
                    best_model_wts = copy.deepcopy(model.state_dict())
                    print(f"‚≠ê Found better model! (Acc: {best_acc:.4f})")

    time_elapsed = time.time() - since
    print(f'\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:4f}')

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πà‡∏≤‡∏á‡∏ó‡∏≠‡∏á (Best Weights)
    model.load_state_dict(best_model_wts)
    return model


# ============================================================
# 4. MAIN
# ============================================================
if __name__ == '__main__':
    dataloaders, class_names, train_size, val_size = load_data()
    dataset_sizes = {'train': train_size, 'val': val_size}

    print("üõ†Ô∏è  Building EfficientNet B0 (Fine-Tuning Mode)...")

    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)

    # üî• KEY CHANGE 1: Unfreeze (‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô)
    for param in model.parameters():
        param.requires_grad = True

        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Layer ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    num_ftrs = model.classifier[1].in_features
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° Dropout ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô Overfitting
    model.classifier = nn.Sequential(
        nn.Dropout(0.4),
        nn.Linear(num_ftrs, 1)
    )

    model = model.to(device)

    criterion = nn.BCEWithLogitsLoss()

    # üî• KEY CHANGE 2: Optimizer & Scheduler
    # ‡πÉ‡∏ä‡πâ LR ‡∏ï‡πà‡∏≥‡πÜ (1e-4) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤ Unfreeze ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ñ‡πâ‡∏≤‡∏™‡∏π‡∏á‡πÑ‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÄ‡∏Å‡πà‡∏≤‡∏à‡∏∞‡∏û‡∏±‡∏á
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

    # ‡∏•‡∏î Learning Rate ‡∏•‡∏á‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ô (‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏à‡∏π‡∏ô‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡πâ‡∏≤‡∏¢)
    # ‡∏•‡∏ö verbose=True ‡∏≠‡∏≠‡∏Å
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

    # Train
    model = train_model(model, criterion, optimizer, scheduler, num_epochs=EPOCHS)

    # Save
    print(f"üíæ Saving BEST model to {MODEL_SAVE_PATH}...")
    torch.save(model, MODEL_SAVE_PATH)
    print("‚úÖ Done! Ready to deploy.")